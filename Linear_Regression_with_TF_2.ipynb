{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression_with_TF_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vochong/Books/blob/master/Linear_Regression_with_TF_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6cEsCu781l",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression with TensorFlow 2.0 APIs\n",
        "This notebook demonstrates how to construct a linear regression model with the newest TensorFlow 2.0 APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNvDqE_3SHNj",
        "colab_type": "text"
      },
      "source": [
        "## Importing the packages\n",
        "\n",
        "First, we need to install [TensorFlow 2.0](https://www.tensorflow.org/alpha/guide/effective_tf2) from PyPI. It's currently available as an alpha release ( as of May 2019 ). Also, we import the [Pandas](https://pandas.pydata.org/) and [NumPy](https://www.numpy.org/) packages along with [Scikit Learn's `train_test_split` module.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-A0vgnmRPXf",
        "colab_type": "code",
        "outputId": "c9834f7d-5168-416e-a3d1-fd885322a612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print( tf.__version__ )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrLriFD3kVB_",
        "colab_type": "text"
      },
      "source": [
        "**Note:**  Installing TensorFlow 2.0 can be an optional step if the Google Colab runtime has it by default. You can first check the TensorFlow version and then install TF 2.0 if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K76_iJ5o97Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install tensorflow==2.0.0-alpha0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSjWLlrOktS_",
        "colab_type": "text"
      },
      "source": [
        "## Processing the Data\n",
        "We download and preprocess our data for training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTPjzPnQlkTv",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the CSV file from GitHub\n",
        "\n",
        "We will download the ZIP archive hosted at [shubham0204/Dataset_Archives](https://github.com/shubham0204/Dataset_Archives) which contains the main CSV file. We are going to use the data from [Graduate Admissions on Kaggle.com](https://www.kaggle.com/mohansacharya/graduate-admissions). That's simple Python!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eIcvmD2l79n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get( 'https://github.com/shubham0204/Dataset_Archives/blob/master/graduate_admission_prediction.zip?raw=true' ) \n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJJvBbHvKVg",
        "colab_type": "text"
      },
      "source": [
        "### Reading the CSV file with Pandas\n",
        "We read the CSV file using [Pandas' `read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function and print the top 5 samples of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VycstAovSbng",
        "colab_type": "code",
        "outputId": "46a4773d-e82d-454a-a97e-f6e0657d2a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "data = pd.read_csv( 'graduate_admission_prediction/Admission_Predict_Ver1.1.csv' )\n",
        "data.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Serial No.  GRE Score  TOEFL Score  ...  CGPA  Research  Chance of Admit \n",
              "0           1        337          118  ...  9.65         1              0.92\n",
              "1           2        324          107  ...  8.87         1              0.76\n",
              "2           3        316          104  ...  8.00         1              0.72\n",
              "3           4        322          110  ...  8.67         1              0.80\n",
              "4           5        314          103  ...  8.21         0              0.65\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmU8ZtivjTe",
        "colab_type": "text"
      },
      "source": [
        "### Processing the CSV\n",
        "\n",
        "We parse the values of the features from the Pandas `data` object. The steps are as follows:\n",
        "\n",
        "1.   Read the values from the numeric/continuous features and normalize them by dividing with 100. ( `continuous_features` ) \n",
        "2.   Read the values from the binary feature. ( `categorical_research_feature` ) \n",
        "3.   Concatenate both the features types to form the final features array. ( `X` )\n",
        "4.   Parse the label values. ( Y ) \n",
        "5.   Split the `X` and `Y` into training and validation datasets.\n",
        "6.   Convert all the arrays to `tf.Tensor` objects.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2UugVvZS_7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "continuous_features = data[ ['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA'] ].values / 100 # ----( 1 )\n",
        "categorical_research_features = data[ [ 'Research' ] ].values # ----( 2 )\n",
        "\n",
        "X = np.concatenate( [ continuous_features , categorical_research_features ] , axis=1 ) # ----( 3 )\n",
        "Y = data[ [ 'Chance of Admit ' ] ].values # ----( 4 )\n",
        "\n",
        "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 ) # ----( 5 )\n",
        "\n",
        "X = tf.constant( train_features , dtype=tf.float32 ) # -----|\n",
        "Y = tf.constant( train_labels , dtype=tf.float32 ) # -------|\n",
        "                                                           #|---( 6 )\n",
        "test_X = tf.constant( test_features , dtype=tf.float32 ) #--|\n",
        "test_Y = tf.constant( test_labels , dtype=tf.float32 ) # ---|\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp8j6c7B1Jq2",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression model\n",
        "\n",
        "We create various methods which help us in creating various components for our [linear regression model](https://machinelearningmastery.com/linear-regression-for-machine-learning/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhOEx2981Xfs",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error and its derivative\n",
        "\n",
        "We define a method which calculates the [mean squared error](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/) given the predictions and the expected outcomes.\n",
        "\n",
        "$\\Large MSE( y , \\hat{y} ) = \\frac{ \\sum_{i=0}^{N} ( y_i - \\hat{y}_i )^2 } { N }$\n",
        "\n",
        "And its derivative,\n",
        "\n",
        "$\\Large MSE'( y , \\hat{y} ) = \\frac{ 2 * \\sum_{i=0}^{N} ( y_i - \\hat{y}_i ) } { N }$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaTYPGTIGwcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def mean_squared_error( Y , y_pred ):\n",
        "    return tf.reduce_mean( tf.square( y_pred - Y ) )\n",
        "\n",
        "def mean_squared_error_deriv( Y , y_pred ):\n",
        "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_4B8k9G2pHf",
        "colab_type": "text"
      },
      "source": [
        "### Hypothesis function (  Regression function )\n",
        "\n",
        "We define the hypothesis function *h* which takes in the weights and the bias and outputs the *Y* value for the given value of *X*.\n",
        "\n",
        "$\\Large h( x ) = W.X + B$\n",
        "\n",
        "where $W$ and $X$ are the parameters which are going to optimize by [Gradient Descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HugDGNhAaF7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def h ( X , weights , bias ):\n",
        "    return tf.tensordot( X , weights , axes=1 ) + bias\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsQQJI6T33Ui",
        "colab_type": "text"
      },
      "source": [
        "### Defining hyperparameters and `tf.data.Dataset`\n",
        "We define two hyperparameters which will affect our training process.\n",
        "\n",
        "*   Batch Size $\\to$ The size of the mini-batch used in mini-batch gradient descent.\n",
        "*   Learning Rate $\\to$ The scalar value which scales the step size of the gradients.\n",
        "\n",
        "Also, for effective mini-batch training, we create a [`tf.data.Dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset) object which holds and batches our data tensors.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3aewwV75USS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "num_epochs = 10\n",
        "num_samples = X.shape[0]\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) \n",
        "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
        "iterator = dataset.__iter__()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oViTe5G75YK7",
        "colab_type": "text"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "We train the model for a given number of epochs. We perform optimization for every batch in our dataset.\n",
        "\n",
        "Forward Propogation, to calculate the loss, is as follows,\n",
        "\n",
        "$\\Large loss = MSE( W.X + b ) $\n",
        "\n",
        "To optimize the parameter $W$, the partial derivative of $MSE$ with respect to $W$ will be,\n",
        "\n",
        "\n",
        "$\\Large \\frac{ \\partial MSE }{ \\partial W} = \\frac{ \\partial MSE }{ \\partial h} \\frac{ \\partial h }{ \\partial W}$\n",
        "\n",
        "Similarly for bias $b$ , \n",
        "\n",
        "$\\Large \\frac{ \\partial MSE }{ \\partial b} = \\frac{ \\partial MSE }{ \\partial h} $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0-QyOc2UhKC",
        "colab_type": "code",
        "outputId": "b4c8ddc6-013e-4890-a695-14565d127033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "\n",
        "num_features = X.shape[1]\n",
        "weights = tf.random.normal( ( num_features , 1 ) ) \n",
        "bias = 0\n",
        "\n",
        "epochs_plot = list()\n",
        "loss_plot = list()\n",
        "\n",
        "for i in range( num_epochs ) :\n",
        "    \n",
        "    epoch_loss = list()\n",
        "    for b in range( int(num_samples/batch_size) ):\n",
        "        x_batch , y_batch = iterator.get_next()\n",
        "   \n",
        "        output = h( x_batch , weights , bias ) \n",
        "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
        "    \n",
        "        dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
        "        dH_dW = x_batch\n",
        "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
        "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
        "    \n",
        "        weights -= ( learning_rate * dJ_dW )\n",
        "        bias -= ( learning_rate * dJ_dB ) \n",
        "        \n",
        "    loss = np.array( epoch_loss ).mean()\n",
        "    epochs_plot.append( i + 1 )\n",
        "    loss_plot.append( loss ) \n",
        "    \n",
        "    print( 'Loss is {}'.format( loss ) ) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss is 19.8784122467041\n",
            "Loss is 9.569950103759766\n",
            "Loss is 4.609400272369385\n",
            "Loss is 2.223372220993042\n",
            "Loss is 1.0768611431121826\n",
            "Loss is 0.5268138647079468\n",
            "Loss is 0.2634977102279663\n",
            "Loss is 0.13798141479492188\n",
            "Loss is 0.0782383531332016\n",
            "Loss is 0.04986153915524483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0vOQ_vN7rnt",
        "colab_type": "text"
      },
      "source": [
        "### Plotting the epoch-loss graph\n",
        "\n",
        "We plot a simple graph which shows us the decrease in loss with increasing epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgKyMat-ujr9",
        "colab_type": "code",
        "outputId": "6410562b-398a-4021-de43-669fc3d24d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot( epochs_plot , loss_plot ) \n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VPd97/H3d2a0IVaBJMRmMGAM\nSIATmXiP8UJAcENuncT2vU2d1inXt87ixm1u4rRN6jxNnbTZm5uEJL52chM3iWMnrgHbxEmNd1vY\nLGIJm7FZJbEjBGj79o85gBASCM1IZ5bP63nmmTnn/GbOl3nsz+/MT+ecn7k7IiKSPSJhFyAiIn1L\nwS8ikmUU/CIiWUbBLyKSZRT8IiJZRsEvIpJlFPwiIllGwS8ikmUU/CIiWSYWdgGdGTZsmI8dOzbs\nMkRE0saKFSv2untxd9qmZPCPHTuW6urqsMsQEUkbZvZ2d9tqqEdEJMso+EVEsoyCX0Qky5w3+M1s\ntJn9wczWmdlaM/tUsL7IzJaZ2abgeUgX778jaLPJzO5I9j9AREQuTHeO+FuAe919CnAFcLeZTQE+\nCzzr7hOBZ4PlM5hZEfAF4D3ATOALXXUQIiLSN84b/O6+293fCF4fAdYDI4EFwMNBs4eBD3Ty9vcB\ny9x9v7sfAJYBc5JRuIiI9MwFjfGb2VjgMuBVoNTddweb9gClnbxlJLC93fKOYJ2IiISk28FvZv2B\nXwP3uPvh9ts8Pn9jQnM4mtlCM6s2s+r6+voLfv/x5lYWLd/Ci5v3JlKGiEjG61bwm1kO8dD/mbs/\nFqyuNbOyYHsZUNfJW3cCo9stjwrWncXdF7l7pbtXFhd36+KzM+REIyxa/hY/f/WdC36viEg26c5Z\nPQb8GFjv7l9vt+kJ4ORZOncAv+3k7U8Ds81sSPBH3dnBuqSLRow55aX8fkMdx5pae2MXIiIZoTtH\n/FcDHwFuMLOVwaMKeAC42cw2ATcFy5hZpZn9CMDd9wNfAl4PHvcH63pFVUUZx5pbeW5jZz8+REQE\nunGvHnd/AbAuNt/YSftq4GPtlh8EHuxpgRdi5tgihhbmsnjNHuaUl/XFLkVE0k5GXbkbi0aYPXU4\nv19fy/FmDfeIiHQmo4IfoKpiOEebWlm+8cLPDBIRyQYZF/xXXDyUIf1yWFqzJ+xSRERSUsYFf040\nwuwpw/ndulpOtGi4R0Sko4wLfoC5FcM5cqKFFzbpYi4RkY4yMvivGj+MgfkxlqzRcI+ISEcZGfy5\nsfjZPcvW7aGppS3sckREUkpGBj/Ez+45fLyFF7douEdEpL2MDf6rJwxjQF6MpWt2n7+xiEgWydjg\nz4tFuWlKKc+sq6W5VcM9IiInZWzwQ/zePQcbm3ll676wSxERSRkZHfzXThxGYW6UJRruERE5JaOD\nPz8nyo2TS3l6bS0tGu4REQEyPPghfnbP/qNNvPZWr90NWkQkrWR88F8/qYR+uVGW1Gi4R0QEsiD4\n83OizLq0hKdqamltS2haYBGRjNCdqRcfNLM6M6tpt+4X7Wbj2mZmK7t47zYzWxO0q05m4ReiqryM\nvQ0neH2bhntERLpzxP8QMKf9Cne/1d1nuPsM4pOwP9bZGwOzgraVPS8zMbMuLSY/J6KLuURE6Ebw\nu/tyoNND5WAi9g8DjyS5rqTqlxtj1qQSltbsoU3DPSKS5RId478WqHX3TV1sd+AZM1thZgsT3FdC\n5laUUXfkBCveORBmGSIioUs0+G/n3Ef717j7u4C5wN1mdl1XDc1soZlVm1l1fX3yp0284dIScmMR\nXcwlIlmvx8FvZjHgT4BfdNXG3XcGz3XA48DMc7Rd5O6V7l5ZXFzc07K61D8vxnsvKeYpDfeISJZL\n5Ij/JmCDu+/obKOZFZrZgJOvgdlATWdt+8q8ijJ2HzrOyh0HwyxDRCRU3Tmd8xHgZWCSme0wszuD\nTbfRYZjHzEaY2ZJgsRR4wcxWAa8Bi939qeSVfuFumFxCbjTCktUa7hGR7BU7XwN3v72L9R/tZN0u\noCp4vRWYnmB9STUwP4drJw5jac0ePj9vMvGTkkREskvGX7nb0dyKMnYePMbqHYfCLkVEJBRZF/w3\nTy4lJ2q6d4+IZK2sC/5B/XK4esIwlqzZjbvO7hGR7JN1wQ/xe/ds33+MtbsOh12KiEify8rgv3lK\nKdGI6WIuEclKWRn8QwpzuWr8UA33iEhWysrgh/hE7Nv2NbJhz5GwSxER6VNZG/yzp5QSMTTcIyJZ\nJ2uDf2j/PK64eCiLNdwjIlkma4Mf4hdzba0/yqa6hrBLERHpM1kd/HOmDsc03CMiWSarg794QB4z\nxxYp+EUkq2R18EP87J6NtQ1srtPZPSKSHbI++OeUx4d7lq7ZE3YpIiJ9IuuDv3RgPpUXDWFJjYJf\nRLJD1gc/wNzyMtbvPsxbe4+GXYqISK/rzgxcD5pZnZnVtFv3RTPbaWYrg0dVF++dY2Z/NLPNZvbZ\nZBaeTHPKhwM6u0dEskN3jvgfAuZ0sv4b7j4jeCzpuNHMosB3gbnAFOB2M5uSSLG9ZcTgAi4bM5il\nuke/iGSB8wa/uy8H9vfgs2cCm919q7s3Af8OLOjB5/SJeRVl1Ow8zDv7GsMuRUSkVyUyxv9xM1sd\nDAUN6WT7SGB7u+UdwbqUdGq4R0f9IpLhehr83wPGAzOA3cDXEi3EzBaaWbWZVdfX1yf6cRds1JB+\nTB81iKUa5xeRDNej4Hf3Wndvdfc24IfEh3U62gmMbrc8KljX1WcucvdKd68sLi7uSVkJm1tRxqod\nh9hxQMM9IpK5ehT8ZlbWbvG/AzWdNHsdmGhm48wsF7gNeKIn++srVeXxf9ZTOqdfRDJYd07nfAR4\nGZhkZjvM7E7gq2a2xsxWA7OAvw7ajjCzJQDu3gJ8HHgaWA/80t3X9tK/IynGDO1H+ciBLNZwj4hk\nsNj5Grj77Z2s/nEXbXcBVe2WlwBnneqZyuaWl/EvT/+RXQePMWJwQdjliIgkna7c7aCqQsM9IpLZ\nFPwdjBtWyOSygbqYS0QyloK/E1Xlw6l++wC1h4+HXYqISNIp+Dsxt6IMdw33iEhmUvB3YkJJfy4p\n7a+btolIRlLwd2FueRmvbdtP/ZETYZciIpJUCv4uzJsWDPes1XCPiGQWBX8XJpb0Z3xxoe7dIyIZ\nR8HfBTOjqqKMV7buY1+DhntEJHMo+M9hbnkZbQ7PrKsNuxQRkaRR8J/D5LIBjBtWqLN7RCSjKPjP\nwcyYWz6cl7bs48DRprDLERFJCgX/eVRVlNHa5izTcI+IZAgF/3lMHTGQ0UUFmpJRRDKGgv88Tp7d\n8+LmvRxqbA67HBGRhCn4u6GqvIzmVmfZeg33iEj6684MXA+aWZ2Z1bRb9y9mtsHMVpvZ42Y2uIv3\nbgtm6lppZtXJLLwvTRs1iJGDC3Qxl4hkhO4c8T8EzOmwbhlQ7u7TgI3A587x/lnuPsPdK3tWYvhO\nnt3z/Ka9HD6u4R4RSW/nDX53Xw7s77DumWBOXYBXgFG9UFtKqZpWRlNrG89quEdE0lwyxvj/Alja\nxTYHnjGzFWa2MAn7Cs2MUYMpG5TPkjW6aZuIpLeEgt/MPg+0AD/rosk17v4uYC5wt5ldd47PWmhm\n1WZWXV9fn0hZvSISMeaUD+e5jfU0nGg5/xtERFJUj4PfzD4KzAf+p7t7Z23cfWfwXAc8Dszs6vPc\nfZG7V7p7ZXFxcU/L6lVVFWU0tbTx+w11YZciItJjPQp+M5sDfAZ4v7s3dtGm0MwGnHwNzAZqOmub\nLt49ZgglA/J0do+IpLXunM75CPAyMMnMdpjZncC/AQOAZcGpmt8P2o4wsyXBW0uBF8xsFfAasNjd\nn+qVf0UfiUTiZ/f84Y91NDZpuEdE0lPsfA3c/fZOVv+4i7a7gKrg9VZgekLVpaC5FWU8/PLb/GFD\nPfOmlYVdjojIBdOVuxfo8rFFDOufp3v3iEjaUvBfoGjEmFNeyh821HGsqTXsckRELpiCvweqysto\nbGrluY06u0dE0o+CvwdmjiuiqDBXF3OJSFpS8PdALBrhfVNLeXZ9LcebNdwjIulFwd9DVRVlHG1q\n5flNe8MuRUTkgij4e+iKi4cyuF+OJmIXkbSj4O+hnGiE2VNK+d26Wk60aLhHRNKHgj8BcyvKOHKi\nhRc3a7hHRNKHgj8BV48fxsD8mM7uEZG0ouBPQG4sws1ThvPM2j00tbSFXY6ISLco+BNUVTGcw8db\neGmLhntEJD0o+BN0zcRh9M+LsVTDPSKSJhT8CcqLRblpcglPr9tDc6uGe0Qk9Sn4k6CqooyDjc28\nsnVf2KWIiJyXgj8JrrukmMLcqM7uEZG00K3gN7MHzazOzGrarSsys2Vmtil4HtLFe+8I2mwyszuS\nVXgqyc+JcsPkUp5Zu4cWDfeISIrr7hH/Q8CcDus+Czzr7hOBZ4PlM5hZEfAF4D3EJ1r/QlcdRLqr\nKh/OvqNNvLZtf9iliIicU7eC392XAx0TbQHwcPD6YeADnbz1fcAyd9/v7geAZZzdgWSE6yeVUJAT\n1b17RCTlJTLGX+ruJ1NuD/HJ1TsaCWxvt7wjWJdxCnKj3HBpCU/V1NLa5mGXIyLSpaT8cdfdHUgo\n7cxsoZlVm1l1fX19Msrqc3MrhrO34QTVGu4RkRSWSPDXmlkZQPDc2TyEO4HR7ZZHBevO4u6L3L3S\n3SuLi4sTKCs8syaVkBeLsLRGZ/eISOpKJPifAE6epXMH8NtO2jwNzDazIcEfdWcH6zJSYV6MWZNK\nWFqzmzYN94hIiuru6ZyPAC8Dk8xsh5ndCTwA3Gxmm4CbgmXMrNLMfgTg7vuBLwGvB4/7g3UZa27F\ncGoPn+DVtzL6nykiacziw/OppbKy0qurq8Muo0eOnmjhhq/9J4MLcnniE1eTF4uGXZKIZAEzW+Hu\nld1pqyt3k6wwL8YDt0zjj7VH+PqyjWGXIyJyFgV/L5g1qYTbZ45h0fKtvK4zfEQkxSj4e8nn501m\n1JAC7v3lKo6eaAm7HBGRUxT8vaR/XoyvfWgG2w808uUl68MuR0TkFAV/L5o5roiPXTOOn736Ds9t\nTM+L0kQk8yj4e9m9sycxsaQ/n3l0FYcam8MuR0REwd/b8nOifP3DM9jX0MQXnqg5/xtERHqZgr8P\nVIwaxMdvmMBvVu5iqe7eKSIhU/D3kbtnTaBi5CDue3wN9UdOhF2OiGQxBX8fyYlG+PqHp3O0qZXP\nPbaGVLxiWkSyg4K/D00sHcBn3jeJ362v5dEVO8IuR0SylIK/j/3F1eOYOa6I+/9jHTsPHgu7HBHJ\nQgr+PhaJGF/70HTa3PnbX63S7ZtFpM8p+EMwuqgffzd/Ci9t2cdPXt4WdjkikmUU/CG57fLRXD+p\nmH9euoEt9Q1hlyMiWUTBHxIz46u3TKMgN8qnf7mKlta2sEsSkSzR4+A3s0lmtrLd47CZ3dOhzfVm\ndqhdm39IvOTMUTIwny8tKGfV9oN8/7ktYZcjIlki1tM3uvsfgRkAZhYlPon64500fd7d5/d0P5nu\nv00fwdNr9/CtZzcx69ISpo4YFHZJIpLhkjXUcyOwxd3fTtLnZZUvLShncL9cPv2LVZxoaQ27HBHJ\ncMkK/tuAR7rYdqWZrTKzpWY2NUn7yyhDCnP5ajBd4zeWbQq7HBHJcAkHv5nlAu8HftXJ5jeAi9x9\nOvAd4Dfn+JyFZlZtZtX19dl37/pZl5Zw2+WjWbR8Cyve1nSNItJ7knHEPxd4w91rO25w98Pu3hC8\nXgLkmNmwzj7E3Re5e6W7VxYXFyehrPTzd/OnMGJwAZ/+5SoamzRdo4j0jmQE/+10McxjZsPNzILX\nM4P97UvCPjNS/7wY//qh6byzv5F/XrIh7HJEJEMlFPxmVgjcDDzWbt1dZnZXsPhBoMbMVgHfBm5z\n3ZbynK64eCh/cfU4fvrK2zy/KfuGvESk91kq5nBlZaVXV1eHXUZojje3Mv87L9BwvIWn77mOQf1y\nwi5JRFKcma1w98rutNWVuykoPl3jdOobTvDF/1gbdjkikmEU/Clq2qjB3D1rAo+/uZOnajRdo4gk\nj4I/hX3ihgmUjxzIfY/XaLpGEUkaBX8Ki0/XOIOGEy3c97imaxSR5FDwp7hLSgfwN7MvYdm6Wn79\nxs6wyxGRDKDgTwN3XnMxM8cW8Y9PrNV0jSKSMAV/GohGjH/90HRa3fnMo5quUUQSo+BPE2OG9uPz\n8ybz4uZ9/PQV3QRVRHpOwZ9G/sfMMbz3kmL+eel6tmq6RhHpIQV/GjEzvvrBaeTFotz7K03XKCI9\no+BPM6UD87l/wVTefOcgP1i+NexyRCQNKfjT0Punj2BeRRnf/N1G1u06HHY5IpJmFPxpyMz40gfK\nGVSQy6d/uVLTNYrIBVHwp6miwly+cksFG/Yc4Zu/03SNItJ9Cv40duPkUj5cOYofPKfpGkWk+xT8\nae7v50+hbFAB92q6RhHppmRMtr7NzNaY2UozO2v2FIv7tpltNrPVZvauRPcppw3Iz+FfPjSNbfsa\neWCppmsUkfNL1hH/LHef0cXsL3OBicFjIfC9JO1TAleNH8afXz2Wn7z8Ni9s2ht2OSKS4vpiqGcB\n8BOPewUYbGZlfbDfrPJ/5lzKxcWF/O2jqzh0rDnsckQkhSUj+B14xsxWmNnCTraPBLa3W94RrJMk\nik/XOIO6Iyf4R03XKCLnkIzgv8bd30V8SOduM7uuJx9iZgvNrNrMquvr65NQVvaZMXowf3X9eB57\nYydPr90TdjkikqISDn533xk81wGPAzM7NNkJjG63PCpY1/FzFrl7pbtXFhcXJ1pW1vrEDROZOmIg\n9z22hr0Nmq5RRM6WUPCbWaGZDTj5GpgN1HRo9gTwZ8HZPVcAh9xds4f3ktxYfLrGI8dbuO8xTdco\nImdL9Ii/FHjBzFYBrwGL3f0pM7vLzO4K2iwBtgKbgR8Cf5XgPuU8Jg0fwL2zL+GZdbXc+XC1JmoX\nkTNYKh4RVlZWenX1WZcEyAVwdx5+aRtfXrqBAXkxvnLLNG6aUhp2WSLSS8xsRRen1J9FV+5mKDPj\no1eP48lPXEPJwHw+9pNq7nt8ja7uFREFf6a7pHQAv7n7Kv7Xey/mkdfeYd63X2DV9oNhlyUiIVLw\nZ4G8WJTPzZ3Mzz92BSeaW7nley/xnWc3aQYvkSyl4M8iV44fytJ7rqOqooyvLdvIrYte4Z19jWGX\nJSJ9TMGfZQYV5PDt2y/jW7fNYGPtEaq+/TyPrtih0z5FsoiCP0stmDGSpZ+6lqkjBvI3v1rF3T9/\ngwNHm8IuS0T6gII/i40a0o+f/+UVfHbupSxbV8ucby3n+U26XYZIplPwZ7loxLjrveN5/K+uZkB+\nDh/58Wvc/x/rON6seXxFMpWCXwAoHzmIJz9xDR+9aiwPvvgW7/+3F1i363DYZYlIL1Dwyyn5OVG+\n+P6pPPTnl3OgsZkPfPdFfrh8K21t+sOvSCZR8MtZrp9UwlOfupbrJxXzT0vW86c/fpXdh46FXZaI\nJImCXzo1tH8eP/jIu/nKLRWs3H6Q931jOU+u3hV2WSKSBAp+6ZKZcevlY1jyyWu5uLg/H//5m3z6\nFys5fFxTO4qkMwW/nNfYYYU8eteV3HPTRH67ahdzv/k8r721P+yyRKSHFPzSLbFohHtuuoRf3XUl\nsahx66KX+epTG2hq0f1+RNKNgl8uyLvGDGHJJ6/l1srR/N//3MKffO9FNtc1hF2WiFyAHge/mY02\nsz+Y2TozW2tmn+qkzfVmdsjMVgaPf0isXEkFhXkxHrhlGj/4yLvZeeAY87/zPD99eZvu9yOSJmIJ\nvLcFuNfd3wjm3V1hZsvcfV2Hds+7+/wE9iMp6n1Th3PZ6MH87aOr+fvfruXZDXV89YPTKBmQH3Zp\nInIOPT7id/fd7v5G8PoIsB4YmazCJD2UDMznoT+/nPsXTOXlLfuY883neWbtnrDLEpFzSMoYv5mN\nBS4DXu1k85VmtsrMlprZ1HN8xkIzqzaz6vp63SgsnZgZf3blWBZ/8hrKBuWz8Kcr+Nxjqzl6QtM8\niqSihCdbN7P+wHPAP7n7Yx22DQTa3L3BzKqAb7n7xPN9piZbT19NLW18fdlGfrB8CxcV9eMbt87g\nsjFDwi5LJOP12WTrZpYD/Br4WcfQB3D3w+7eELxeAuSY2bBE9impLTcW4bNzL+WRv7yC5lbng9+P\nn/a5fb9m+hJJFT0+4jczAx4G9rv7PV20GQ7Uurub2UzgUeAiP89OdcSfGQ4da+YLv63hNyvjt3qY\nMXow86eVUVVRxojBBSFXJ5JZLuSIP5HgvwZ4HlgDnLyK5z5gDIC7f9/MPg78b+JnAB0DPu3uL53v\nsxX8mWX7/kYWr9nNk6t3UbMzfqvnd1805FQnUDpQZwGJJKpPgr83Kfgz17a9R4NOYDfrdx/GDC4f\nW8T8aWXMLS+jeEBe2CWKpCUFv6SFLfUNLF4d/yWwsbaBiMF7xg1l/vQy5kwdztD+6gREukvBL2ln\nY+0Rngw6ga31R4lGjKvGD2VeRRlzyoczuF9u2CWKpDQFv6Qtd2fDniOnfgls29dILGJcM3EY8yrK\nmD11OIMKcsIuUyTlKPglI7g7a3cd5snVu1m8Zhfb9x8jJ2pcN7GY+dPLuGlyKQPy1QmIgIJfMpC7\ns3rHIRav2c3i1bvZefAYubEI119SzLxp8U6gMC+RW0+JpDcFv2S0tjbnze0HWbx6N0vW7GbP4ePk\nxSLccGkJ86eNYNalxfTLVScg2UXBL1mjrc1Z8c4Bnly1iyU1e6g/coKCnCg3Ti5h/rQyrp9UQn5O\nNOwyRXqdgl+yUmub89pb+1m8ZhdL1+xh39EmCnOj3DSllPnTRnDtxGHqBCRjKfgl67W0tvHqW/t5\ncvUunqrZw4HGZiIGFw0tZEJJfyaU9Gdi8Dy+uL/+PiBpT8Ev0k5zaxsvbdnHim372VzfwKbaBrbt\nO0pz6+n/9kcOLjijM5hY2p8JxQMY1E9nDUl6uJDg12GOZLycaIT3XlLMey8pPrWuubWNt/c1srmu\ngc11R9hc18CmugZefWsfx5tPTyBfPCCPCcVBR1By+lHcP4/4fQpF0o+CX7JSTjRyKsRh+Kn1bW3O\nzoPHgo7gdIfw+Bs7OdJuYplBBTln/EI4+RgxqIBIRB2CpDYN9Yh0g7tTd+QEm2rjvxA21TUEvxYa\n2He06VS7frlRxhfHO4Tx7TqGMUX9iEWTMuGdSKc01COSZGZG6cB8Sgfmc83EM+cS2n+06VQncPJX\nwstb9/HYmztPtcmNRhg3rJAJpf0ZObiAIf1yKSrMYUi/XIYU5gbLuQwqyCGqXwzSyxT8IgkqKsxl\n5rgiZo4rOmP9kePNbKk/eqpD2FLXQM3OQzy7vvaMvyO0ZwaDC3IYUphL0alO4czlU8/BtoH5ORpe\nkgui4BfpJQPyc5gxejAzRg8+a9uxplYONDax/2jT6eejTexvbOZAsO5AYxM7DhxjzY5D7D/aRFNr\n551FxDj1yyHeKeRQVJjL4H7tO4mcU78qhhTmMiAvpj9OZ7GEgt/M5gDfAqLAj9z9gQ7b84CfAO8G\n9gG3uvu2RPYpkgkKcqMU5BZ0ewpKd6exqfVUR3Eg6CDO6DiC5217G3nznYMcaGw645TV9mIRi9eQ\nEyU/5+RzhPwOywW5UfJiUQpyo+THTq/Lj0XJz42SHwuWu/yMqIauUlCPg9/MosB3gZuBHcDrZvaE\nu69r1+xO4IC7TzCz24CvALcmUrBINjIzCvNiFObFGF3Ur1vvcXcaTrRw4Ggz+xubzugoDjQ2cfRE\nKydaWjnW1Mrx5jaONbdyvLmVg41N7Gm3fKy5lRPNbV3+4jif3GiEvJzIqY4gP3idFyznRo1oxIhF\nI8QiRiwSPEcteD69HI1EyIkY0aiRE4kQjRg5wfrTbYyc6JnbciId9hEN9hPsIxo8ImaYQcTir6Nm\nWOTkMmdsjwav0/GXUyJH/DOBze6+FcDM/h1YALQP/gXAF4PXjwL/ZmZ2vsnWRSRxZsaA/BwG5Ocw\nZmj3OotzaW3zUx3B8VOPtnbr2jpsa+VYUxvHg86ls07mUGMTLW1OS6vT0tZ2xuvWNqe51YPn+HJL\nW+pFx+mOIv6dRwyi1q4TCTqUk9uj7dtGTncyZjCsMI9f3nVlr9ecSPCPBLa3W94BvKerNu7eYmaH\ngKHA3o4fZmYLgYUAY8aMSaAsEekN0cjpXx1hcfdTHUC8kzizs4g/n/m6ta3trA7k5PLpdm24Q5tD\nm/up/Zxejj+fXG47Y1u79efd7rS1nW7rJ9c5tLozoI++25T54667LwIWQfw8/pDLEZEUZBYM0+he\newlJ5IqSncDodsujgnWdtjGzGDCI+B95RUQkJIkE/+vARDMbZ2a5wG3AEx3aPAHcEbz+IPB7je+L\niISrx0M9wZj9x4GniZ/O+aC7rzWz+4Fqd38C+DHwUzPbDOwn3jmIiEiIEhrjd/clwJIO6/6h3evj\nwIcS2YeIiCSX7holIpJlFPwiIllGwS8ikmUU/CIiWSYlJ2Ixs3rg7bDrSNAwOrlCOUvpuziTvo8z\n6fs4LZHv4iJ3Lz5/sxQN/kxgZtXdnQ0n0+m7OJO+jzPp+zitr74LDfWIiGQZBb+ISJZR8PeeRWEX\nkEL0XZxJ38eZ9H2c1iffhcb4RUSyjI74RUSyjII/icxstJn9wczWmdlaM/tU2DWlAjOLmtmbZvZk\n2LWEycwGm9mjZrbBzNabWe9PtZTCzOyvg/9PaszsETPLD7umvmRmD5pZnZnVtFtXZGbLzGxT8Dyk\nN/at4E+uFuBed58CXAHcbWZTQq4pFXwKWB92ESngW8BT7n4pMJ0s/k7MbCTwSaDS3cuJ3+E32+7e\n+xAwp8O6zwLPuvtE4NlgOekU/Enk7rvd/Y3g9RHi/2OPDLeqcJnZKGAe8KOwawmTmQ0CriN+q3Lc\nvcndD4ZbVehiQEEwSVM/YFfI9fQpd19O/Hb17S0AHg5ePwx8oDf2reDvJWY2FrgMeDXcSkL3TeAz\nQFvYhYRsHFAP/L9g2OtHZlYYdlFhcfedwL8C7wC7gUPu/ky4VaWEUnffHbzeA5T2xk4U/L3AzPoD\nvwbucffDYdcTFjObD9S5+4rQsSTuAAABQUlEQVSwa0kBMeBdwPfc/TLgKL30Mz4dBGPXC4h3iCOA\nQjP703CrSi3BbIW9ctqlgj/JzCyHeOj/zN0fC7uekF0NvN/MtgH/DtxgZv8/3JJCswPY4e4nfwE+\nSrwjyFY3AW+5e727NwOPAVeFXFMqqDWzMoDgua43dqLgTyIzM+JjuOvd/eth1xM2d/+cu49y97HE\n/3D3e3fPyqM6d98DbDezScGqG4F1IZYUtneAK8ysX/D/zY1k8R+722k/T/kdwG97YycK/uS6GvgI\n8SPblcGjKuyiJGV8AviZma0GZgBfDrme0AS/fB4F3gDWEM+irLqC18weAV4GJpnZDjO7E3gAuNnM\nNhH/VfRAr+xbV+6KiGQXHfGLiGQZBb+ISJZR8IuIZBkFv4hIllHwi4hkGQW/iEiWUfCLiGQZBb+I\nSJb5L7hn/hSRAA+2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNFROc6M8jQX",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the model\n",
        "\n",
        "We evaluate our model's performance by measuring the [Mean Absolute Error](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html?highlight=mean%20absolute#mae-l1) on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyKcSFJrbhRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "output = h( test_X , weights , bias ) \n",
        "labels = test_Y\n",
        "\n",
        "accuracy_op = tf.metrics.MeanAbsoluteError() \n",
        "accuracy_op.update_state( labels , output )\n",
        "print( 'Mean Absolute Error = {}'.format( accuracy_op.result().numpy() ) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}